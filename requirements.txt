torch
vllm
transformers
numpy<=2.2 # vllm fails for numpy >= 2.3 on B200s
omegaconf
datasets
flashinfer-python # optional, for faster vLLM sampling